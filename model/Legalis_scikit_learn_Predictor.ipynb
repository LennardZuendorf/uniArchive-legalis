{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Legalis SKOPS Predictor\n",
        "#### Notebook used to create and publish an sckit-learn text classification modek for outcome prediction"
      ],
      "metadata": {
        "datalore": {
          "node_id": "nF2mGuJOMONr7IOOnJaqgK",
          "type": "MD",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "ZchUh6jJy9Eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libary and Dataset Import"
      ],
      "metadata": {
        "datalore": {
          "node_id": "VnMhcEega3QtGpRfaBhXc0",
          "type": "MD",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "1B7gnW-ty9Et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install skops\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "j1Syx16py-OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports for utility\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#nltk imports for stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#sklearn import for model creation\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import (\n",
        "    ConfusionMatrixDisplay,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#imports to pull/push to/from huggingface hub and persistence\n",
        "import datasets as ds\n",
        "from skops.io import dump, load\n",
        "from skops import hub_utils, card\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "import huggingface_hub\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "up9pfJwvUcKR9Dl437lzDH",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "ExecuteTime": {
          "end_time": "2023-06-25T19:51:38.866619400Z",
          "start_time": "2023-06-25T19:51:38.835406300Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "870-a6Dcy9Eu",
        "outputId": "734686e3-21c9-4694-fb0a-c139cc81ece2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### preprocessing dataset again (stripping down and changing names)"
      ],
      "metadata": {
        "datalore": {
          "node_id": "Zxl7hLzKOu1K5c8MNMWOGY",
          "type": "MD",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "bxuBHpWSy9Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hgf_dataset=ds.load_dataset(\"lennardzuendorf/legalis\")\n",
        "hgf_dataset=hgf_dataset.remove_columns(['id', 'file_number', 'date', 'type', 'content', 'tenor','reasoning'])\n",
        "hgf_dataset=hgf_dataset.rename_column('facts', 'text')\n",
        "hgf_dataset=hgf_dataset.rename_column('winner', 'target')\n",
        "\n",
        "print(hgf_dataset)\n",
        "\n",
        "dataset=ds.concatenate_datasets([hgf_dataset['train'], hgf_dataset['test']])\n",
        "\n",
        "print(dataset)\n",
        "print(dataset[1]['target'])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'target', 'label'],\n",
            "        num_rows: 2660\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'target', 'label'],\n",
            "        num_rows: 141\n",
            "    })\n",
            "})\n",
            "Dataset({\n",
            "    features: ['text', 'target', 'label'],\n",
            "    num_rows: 2801\n",
            "})\n",
            "Klaeger*in\n"
          ]
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "BbNnDZmR1irh5OodjgeIrb",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "ExecuteTime": {
          "end_time": "2023-06-25T19:09:45.080153100Z",
          "start_time": "2023-06-25T19:09:22.681521200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VgSY_l-y9Ex",
        "outputId": "dcffe338-fb27-40e7-9cbc-f69326e92d97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_dist(dataset):\n",
        "\n",
        "    counter_zero=0\n",
        "    counter_one=0\n",
        "\n",
        "    for case in dataset:\n",
        "        if case['label']==1:\n",
        "            counter_one+=1\n",
        "        elif case['label']==0:\n",
        "            counter_zero+=1\n",
        "\n",
        "    dist=[round(counter_zero/(counter_zero+counter_one),2),round(counter_one/(counter_zero+counter_one),2)]\n",
        "\n",
        "    data={'Verklagte*r': [counter_zero, dist[0]], 'Klaeger*in': [counter_one, dist[1]]}\n",
        "    index=['case nmb.', 'case dist.']\n",
        "\n",
        "    return pd.DataFrame(data=data, index=index)"
      ],
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "Mb8UoIGmDeONRfdU5a7lQ4",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "HZTwpI7Hy9Ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predifining Function to run several times with different dataset sizes\n",
        "#### function for dataframe creation with variable size and splitting into test, train"
      ],
      "metadata": {
        "datalore": {
          "node_id": "djVKj5kXkGJRejZssM3GgG",
          "type": "MD",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "4127772wy9Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(size, test_size):\n",
        "\n",
        "  df_dataset=pd.DataFrame(dataset[:size])\n",
        "  df_train, df_test = train_test_split(df_dataset, test_size=test_size)\n",
        "\n",
        "  return df_train, df_test"
      ],
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "SHQhU0R8huHDrZJiCsPchn",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "ExecuteTime": {
          "end_time": "2023-06-25T19:10:10.260126800Z",
          "start_time": "2023-06-25T19:10:10.249538300Z"
        },
        "id": "WI9E6uOWy9Ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### function to vectorize words and get rid of german stopwords with nltk"
      ],
      "metadata": {
        "datalore": {
          "node_id": "DTHwG3e44f9ppT0wEX7voB",
          "type": "MD",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "YvgkzYdIy9Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=stopwords.words('german')\n",
        "\n",
        "def vectorize_words_stop(df_train, df_test):\n",
        "\n",
        "    vec = CountVectorizer(\n",
        "        ngram_range=(1, 3),\n",
        "        stop_words=stop_words,\n",
        "    )\n",
        "\n",
        "    X_train = vec.fit_transform(df_train.text)\n",
        "    X_test = vec.transform(df_test.text)\n",
        "\n",
        "    y_train = df_train.target\n",
        "    y_test = df_test.target\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "DaXfAMej7GnNhYjmxwO2tK",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "ExecuteTime": {
          "end_time": "2023-06-25T19:10:12.368445600Z",
          "start_time": "2023-06-25T19:10:12.314290800Z"
        },
        "id": "78UCap_yy9Ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### block for fitting models and returning reports"
      ],
      "metadata": {
        "datalore": {
          "node_id": "2HdI2a6lNDCKU9J0KlKacx",
          "type": "MD",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "1Ch5C6zay9E0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mulitmodel_fitter(X_train, y_train, X_test, y_test):\n",
        "  nb = MultinomialNB()\n",
        "  nb.fit(X_train, y_train)\n",
        "\n",
        "  predict = nb.predict(X_test)\n",
        "  return classification_report(y_test, predict, output_dict=True)\n",
        "\n",
        "def forest_fitter(X_train, y_train, X_test, y_test, config):\n",
        "  rf = RandomForestClassifier(random_state=0, **config)\n",
        "  rf.fit(X_train, y_train)\n",
        "\n",
        "  predict = rf.predict(X_test)\n",
        "  return  classification_report(y_test, predict, output_dict=True)\n",
        "\n",
        "def forest_model_creator(X_train, y_train, X_test, y_test, config):\n",
        "  rf = RandomForestClassifier(random_state=0, **config)\n",
        "  rf.fit(X_train, y_train)\n",
        "\n",
        "  predict = rf.predict(X_test)\n",
        "  return  rf, classification_report(y_test, predict, output_dict=True)"
      ],
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "EUqmmltmZ4bHHRQ49qWhs9",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "ExecuteTime": {
          "end_time": "2023-06-25T19:10:15.547381Z",
          "start_time": "2023-06-25T19:10:15.528973800Z"
        },
        "id": "C1PAHg40y9E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### block for the random forest tuning function"
      ],
      "metadata": {
        "id": "Dl_3dtS6lKFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forest_tuner(X_train, y_train, n_iter, cv, random_state):\n",
        "\n",
        "  param_grid = {\n",
        "      'n_estimators': [100, 200, 300],\n",
        "      'max_depth': [None, 5, 10, 15],\n",
        "      'min_samples_split': [2, 5, 10],\n",
        "      'min_samples_leaf': [1, 2, 4],\n",
        "      'max_features': ['log2', 'sqrt'],\n",
        "      'bootstrap': [True, False],\n",
        "  }\n",
        "\n",
        "  random_search = RandomizedSearchCV(\n",
        "      estimator=RandomForestClassifier(),\n",
        "      param_distributions=param_grid,\n",
        "      n_iter=n_iter,\n",
        "      cv=cv,\n",
        "      random_state=random_state\n",
        "  )\n",
        "\n",
        "  random_search.fit(X_train, y_train)\n",
        "  return random_search.best_params_, random_search.cv_results_"
      ],
      "metadata": {
        "id": "ToMbH0TBSFiX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Model with different dataset sizes\n",
        "#### functions to run model with different sizes or with/without stop words and extract, plot stats from it"
      ],
      "metadata": {
        "datalore": {
          "node_id": "wWj50xauUlmDXAYKPiFvBb",
          "type": "MD",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "_XSTBzx7y9E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mutimodal_runner(run_list):\n",
        "  output=[]\n",
        "\n",
        "  for run in run_list:\n",
        "    df_train, df_test = create_dataset(run, 0.2)\n",
        "    X_train, X_test, y_train, y_test = vectorize_words_stop(df_train, df_test)\n",
        "    report = mulitmodel_fitter(X_train, y_train, X_test, y_test)\n",
        "    output.append(report)\n",
        "\n",
        "  return output\n",
        "\n",
        "def forest_runner(run_list, n_iter, cv, random_state):\n",
        "  tuning_results=[]\n",
        "  fitting_results=[]\n",
        "\n",
        "  for run in run_list:\n",
        "    df_train, df_test = create_dataset(run, 0.2)\n",
        "    X_train, X_test, y_train, y_test = vectorize_words_stop(df_train, df_test)\n",
        "\n",
        "    config, results=forest_tuner(X_train, y_train, n_iter, cv, random_state)\n",
        "    tuning_results.append(results)\n",
        "\n",
        "    report = forest_fitter(X_train, y_train, X_test, y_test, config)\n",
        "    fitting_results.append(report)\n",
        "\n",
        "\n",
        "  return tuning_results, fitting_results"
      ],
      "metadata": {
        "id": "rNxg0OCmlR-N"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stat_extractor(run_stats, run_nmbs):\n",
        "  data={}\n",
        "  index=['precision Klaeger*in', 'precision Verklagte*r', 'overall accuracy', 'macro avg', 'weighted avg']\n",
        "\n",
        "  i=0\n",
        "\n",
        "  for run in run_stats:\n",
        "    df=pd.DataFrame.from_dict(run)\n",
        "    values = df.values[:1][0].round(decimals=3)\n",
        "    run_data={str(run_nmbs[i]): values}\n",
        "    data.update(run_data)\n",
        "    i=i+1\n",
        "\n",
        "  return pd.DataFrame(data=data, index=index)\n",
        "\n",
        "def fitting_plotter(run_stats, mode):\n",
        "  df = run_stats\n",
        "  df_filtered = run_stats.drop(\"macro avg\")\n",
        "  if mode=='multimodal':\n",
        "    title='naive bayes multimodal performance with different dataset sizes'\n",
        "  else:\n",
        "    title='random forest performance with different dataset sizes'\n",
        "\n",
        "  # Create a figure and subplots\n",
        "  fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
        "  fig.suptitle(title, fontsize=16)\n",
        "  fig.tight_layout(pad=3.0)  # Adjust the padding between subplots\n",
        "  axs = axs.flatten()\n",
        "\n",
        "  # creat subplot for each of the 4 left over metrics\n",
        "  for i, metric in enumerate(df_filtered.index[:4]):\n",
        "      axs[i].plot(df.columns, df_filtered.loc[metric], marker='o')\n",
        "      axs[i].set_title(metric)\n",
        "\n",
        "  # set labels\n",
        "  for ax in axs:\n",
        "      ax.set_xlabel('nmb. of cases')\n",
        "  axs[0].set_ylabel('Precision')\n",
        "  axs[1].set_ylabel('Accuracy')\n",
        "\n",
        "  # Display the plot\n",
        "  plt.show()\n",
        "\n",
        "def tuning_plotter(cv_results_list, run_nmbs):\n",
        "    num_runs = len(cv_results_list)\n",
        "    num_plots = num_runs\n",
        "\n",
        "    # Determine the number of rows and columns for subplots\n",
        "    num_rows = int(np.ceil(np.sqrt(num_plots)))\n",
        "    num_cols = int(np.ceil(num_plots / num_rows))\n",
        "\n",
        "    # Create the subplots\n",
        "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
        "    fig.suptitle(f\"search test scores across {num_plots} runs with different dataset sizes\", fontsize=16)\n",
        "    fig.tight_layout(pad=3.0)\n",
        "\n",
        "    # Flatten the axis array if there is only one row or column\n",
        "    if num_plots == 1:\n",
        "        axs = np.array([axs])\n",
        "\n",
        "    # Iterate over the cv_results list and plot the test scores\n",
        "    for i, cv_results in enumerate(cv_results_list):\n",
        "        # Extract the test scores\n",
        "        test_scores = cv_results['mean_test_score']\n",
        "\n",
        "        # Create the x-axis values (iteration index)\n",
        "        iteration_index = np.arange(1, len(test_scores) + 1)\n",
        "\n",
        "        # Determine the subplot location\n",
        "        row = i // num_cols\n",
        "        col = i % num_cols\n",
        "\n",
        "        # Create a scatter plot in the corresponding subplot\n",
        "        axs[row, col].scatter(iteration_index, test_scores)\n",
        "        axs[row, col].set_xticks(range(0, len(test_scores) + 1, 1))\n",
        "        axs[row, col].set_xlabel('Iteration')\n",
        "        axs[row, col].set_ylabel('Test Score')\n",
        "        axs[row, col].set_title(f'Run with {run_nmbs[i]} cases')\n",
        "\n",
        "    # Remove any empty subplots\n",
        "    if num_plots < num_rows * num_cols:\n",
        "        for i in range(num_plots, num_rows * num_cols):\n",
        "            row = i // num_cols\n",
        "            col = i % num_cols\n",
        "            fig.delaxes(axs[row, col])\n",
        "\n",
        "    # Adjust the spacing between subplots\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "execution_count": 39,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "xXv1sVioC1QZZjADG9zNHm",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "ExecuteTime": {
          "end_time": "2023-06-25T19:56:06.469028200Z",
          "start_time": "2023-06-25T19:56:06.453007300Z"
        },
        "id": "ppVnn-kmy9E1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### running and displaying stats for multimodal model with different dataset sizes"
      ],
      "metadata": {
        "id": "kzXp2Z0ERpZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_nmbs=[1000, 2000, 2400, 2800]\n",
        "multimodal_run_stats=mutimodal_runner(run_nmbs)\n",
        "multimodal_run_stats=stat_extractor(multimodal_run_stats, run_nmbs)\n",
        "\n",
        "display(multimodal_run_stats)\n",
        "fitting_plotter(multimodal_run_stats, mode=\"multimodal\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9dab8aa8c4a7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrun_nmbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2800\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmultimodal_run_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmutimodal_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_nmbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmultimodal_run_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstat_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultimodal_run_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_nmbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultimodal_run_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-fdc913fe281c>\u001b[0m in \u001b[0;36mmutimodal_runner\u001b[0;34m(run_list)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_words_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulitmodel_fitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-72272164aedc>\u001b[0m in \u001b[0;36mvectorize_words_stop\u001b[0;34m(df_train, df_test)\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1403\u001b[0m             )\n\u001b[1;32m   1404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_sort_features\u001b[0;34m(self, X, vocabulary)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mreordered\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodifies\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \"\"\"\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0msorted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m         \u001b[0mmap_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "2G7IaZYdCemA9eZL8ARiXb",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "ExecuteTime": {
          "end_time": "2023-06-25T20:29:07.718741400Z",
          "start_time": "2023-06-25T20:28:27.875132Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6jU60kH_y9E1",
        "outputId": "7a61d4eb-5010-4b10-ec79-ef14bdd5ce6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tuning and running random forest classifier for different data sizes, plotting results"
      ],
      "metadata": {
        "id": "XHHmcYsnckLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_nmbs=[1000, 2000, 2400, 2800]\n",
        "\n",
        "tuning_results, fitting_results=forest_runner(run_nmbs, n_iter=10, cv=2, random_state=42)\n",
        "fitting_results=stat_extractor(fitting_results, run_nmbs)\n",
        "\n",
        "print(fitting_results)\n",
        "print(\"\\n\")\n",
        "fitting_plotter(fitting_results, mode=\"forest\")\n",
        "print(\"\\n\")\n",
        "tuning_plotter(tuning_results, run_nmbs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "NGkb3uZQRy9j",
        "outputId": "2bcb9b79-db8b-4435-d86c-30950634e8f8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-c31598bcc66a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrun_nmbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2800\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtuning_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitting_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforest_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_nmbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfitting_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstat_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitting_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_nmbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-fdc913fe281c>\u001b[0m in \u001b[0;36mforest_runner\u001b[0;34m(run_list, n_iter, cv, random_state)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_words_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforest_tuner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-72272164aedc>\u001b[0m in \u001b[0;36mvectorize_words_stop\u001b[0;34m(df_train, df_test)\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1403\u001b[0m             )\n\u001b[1;32m   1404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_sort_features\u001b[0;34m(self, X, vocabulary)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mreordered\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodifies\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \"\"\"\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0msorted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m         \u001b[0mmap_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### training, outputting a model with the maximum data size and based on the best configuration"
      ],
      "metadata": {
        "id": "tOYYgJXTVzrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_model_trainer(dataset_size, test_size,n_iter, cv, random_state):\n",
        "\n",
        "  df_train, df_test = create_dataset(dataset_size, test_size)\n",
        "  X_train, X_test, y_train, y_test = vectorize_words_stop(df_train, df_test)\n",
        "  best_config, cv_report = forest_tuner(X_train, y_train, n_iter, cv, random_state)\n",
        "  model, fitting_report = forest_model_creator(X_train, y_train, X_test, y_test, best_config)\n",
        "\n",
        "  return model, X_test, y_test"
      ],
      "metadata": {
        "id": "C1zaoTaBTvnf"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Persistence and Creating a Pipeline\n",
        "#### searching for the for best classifier config with the maximum dataset size and saving in with skops to host on huggingface"
      ],
      "metadata": {
        "id": "SGkZRj73eDIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=stopwords.words('german')\n",
        "\n",
        "def pipeline_creator(dataset_size,test_size,n_iter,cv,random_state):\n",
        "\n",
        "  df_train, df_test = create_dataset(dataset_size, test_size)\n",
        "  X_train, X_test, y_train, y_test = vectorize_words_stop(df_train, df_test)\n",
        "  best_config, cv_report = forest_tuner(X_train, y_train, n_iter, cv, random_state)\n",
        "\n",
        "  df_dataset=pd.DataFrame(dataset[:dataset_size])\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_dataset.text, df_dataset.target, test_size=test_size, random_state=random_state\n",
        "  )\n",
        "\n",
        "  pipe = Pipeline(\n",
        "    [\n",
        "        (\"count\", CountVectorizer(ngram_range=(1, 3),stop_words=stop_words,)),\n",
        "        (\"clf\", RandomForestClassifier(random_state=0, **best_config)),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  pipe.fit(X_train, y_train)\n",
        "\n",
        "  return pipe, X_test, y_test"
      ],
      "metadata": {
        "id": "jXwQJPlEcy_W"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_saver(model, format):\n",
        "  file_name=\"legalis-scikit\"\n",
        "\n",
        "  # creating example data from dataset\n",
        "  hub_data=[]\n",
        "\n",
        "  for i in range(3):\n",
        "    hub_data.append(dataset[i]['text'])\n",
        "\n",
        "  if format==\"pickle\":\n",
        "    #saving model as pickle file\n",
        "    model_path=f\"{file_name}.pkl\"\n",
        "    repo_path=f\"{file_name}-pickle\"\n",
        "\n",
        "    with open(model_path, mode=\"bw\") as f:\n",
        "      pickle.dump(model, file=f)\n",
        "\n",
        "  elif format == \"skops\":\n",
        "    # saving model as skops file\n",
        "    model_path=f\"{file_name}.skops\"\n",
        "    repo_path=f\"{file_name}-skops\"\n",
        "    dump(model,model_path)\n",
        "\n",
        "  # creating config file\n",
        "  hub_utils.init(\n",
        "      model=model_path,\n",
        "      requirements=[f\"scikit-learn={sklearn.__version__}\"],\n",
        "      dst=repo_path,\n",
        "      task='text-classification',\n",
        "      data=hub_data\n",
        "  )"
      ],
      "metadata": {
        "id": "J3QZmMR089TO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hub_create_save(format, model, X_test, y_test,token, repo_id, message):\n",
        "  file_name=\"legalis-scikit\"\n",
        "\n",
        "  if format == \"pickle\":\n",
        "    repo_path=f\"{file_name}-pickle\"\n",
        "    get_started_code = \"import pickle \\nwith open(dtc_pkl_filename, 'rb') as file: \\n    clf = pickle.load(file)\"\n",
        "\n",
        "  elif format == \"skops\":\n",
        "    repo_path=f\"{file_name}-skops\"\n",
        "    get_started_code = \"from skops.io import load \\n clf = load('my-model.skops', trusted=True)\"\n",
        "\n",
        "  # create the card\n",
        "  model_card = card.Card(model, metadata=card.metadata_from_config(Path(repo_path)))\n",
        "\n",
        "  limitations = \"This model is not ready to be used in production.\"\n",
        "  model_description = \"This is a RandomForestClassifier model trained a dataset of legal cases to predict the outcome as a binary value.\"\n",
        "  model_card_authors = \"LennardZuendorf\"\n",
        "\n",
        "  # we can add the information using add\n",
        "  model_card.add(\n",
        "      get_started_code=get_started_code,\n",
        "      model_card_authors=model_card_authors,\n",
        "      limitations=limitations,\n",
        "      model_description=model_description,\n",
        "  )\n",
        "\n",
        "  # we can set the metadata part directly\n",
        "  model_card.metadata.license = \"mit\"\n",
        "\n",
        "  # let's make a prediction and evaluate the model\n",
        "  y_pred = model.predict(X_test)\n",
        "  # we can pass metrics using add_metrics and pass details with add\n",
        "  model_card.add(eval_method=\"The model is evaluated using test split, on accuracy and F1 score with macro average.\")\n",
        "  model_card.add_metrics(accuracy=accuracy_score(y_test, y_pred))\n",
        "  model_card.add_metrics(**{\"f1 score\": f1_score(y_test, y_pred, average=\"micro\")})\n",
        "\n",
        "  model_card.save(Path(repo_path) / \"README.md\")\n",
        "\n",
        "  hub_utils.push(\n",
        "    repo_id=repo_id,\n",
        "    source=repo_path,\n",
        "    token=token,\n",
        "    commit_message=message,\n",
        "    create_remote=True,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "LOLtl4BaBlrx"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a single model to safe without vectorization\n",
        "model_save_single,X_test_save, y_test_save = best_model_trainer(dataset_size=2800, test_size=0.2, n_iter=10, cv=2, random_state=42)\n",
        "\n",
        "#creating a model with pipeline to include vectorization for interactivity\n",
        "model_save_pipeline,X_test_save, y_test_save = pipeline_creator(dataset_size=2800, test_size=0.2, n_iter=10, cv=2, random_state=42)"
      ],
      "metadata": {
        "id": "qtzOpEdDM1gj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_saver(model=model_save_pipeline,format=\"pickle\")\n",
        "hub_create_save(format=\"pickle\", model=model_save_pipeline, X_test=X_test_save, y_test=y_test_save, token = \"\", repo_id=\"LennardZuendorf/legalis-scikit\", message=\"Updating to use pipeline with included vectorizer.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c05f7292767a42c1b9e1937d26ae3e80",
            "adce098848c64ad69101691e4d66bd5c",
            "2b0db6d55855439e812f46586fdd91b2",
            "5a49efb3e9bd4b6db75820aac9cc9e17",
            "f60a9a14bff14725b3c7a7df7923e610",
            "e3ede82afc4e44e9be531c7b8c29ac3b",
            "15d8bc7dcfb1422cb2c2673aea548ff7",
            "438a71719bf3452190576be398db254a",
            "a3a7d8c560e44d6faf2d4c065062a27e",
            "c7c0cb85d7bc4e4bb93c66c7a1679368",
            "201305d64ae7461d9df1919bd2ef5a84"
          ]
        },
        "id": "XzpssDboe_TF",
        "outputId": "51e7fbd3-2277-42c6-dfbc-6a6042de5dec"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "legalis-scikit.pkl:   0%|          | 0.00/83.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c05f7292767a42c1b9e1937d26ae3e80"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c05f7292767a42c1b9e1937d26ae3e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adce098848c64ad69101691e4d66bd5c",
              "IPY_MODEL_2b0db6d55855439e812f46586fdd91b2",
              "IPY_MODEL_5a49efb3e9bd4b6db75820aac9cc9e17"
            ],
            "layout": "IPY_MODEL_f60a9a14bff14725b3c7a7df7923e610"
          }
        },
        "adce098848c64ad69101691e4d66bd5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3ede82afc4e44e9be531c7b8c29ac3b",
            "placeholder": "​",
            "style": "IPY_MODEL_15d8bc7dcfb1422cb2c2673aea548ff7",
            "value": "legalis-scikit.pkl: 100%"
          }
        },
        "2b0db6d55855439e812f46586fdd91b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_438a71719bf3452190576be398db254a",
            "max": 83474777,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3a7d8c560e44d6faf2d4c065062a27e",
            "value": 83474777
          }
        },
        "5a49efb3e9bd4b6db75820aac9cc9e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7c0cb85d7bc4e4bb93c66c7a1679368",
            "placeholder": "​",
            "style": "IPY_MODEL_201305d64ae7461d9df1919bd2ef5a84",
            "value": " 83.5M/83.5M [00:01&lt;00:00, 46.0MB/s]"
          }
        },
        "f60a9a14bff14725b3c7a7df7923e610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ede82afc4e44e9be531c7b8c29ac3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d8bc7dcfb1422cb2c2673aea548ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "438a71719bf3452190576be398db254a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3a7d8c560e44d6faf2d4c065062a27e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7c0cb85d7bc4e4bb93c66c7a1679368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "201305d64ae7461d9df1919bd2ef5a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "datalore": {
      "computation_mode": "JUPYTER",
      "package_manager": "pip",
      "base_environment": "default",
      "packages": [
        {
          "name": "datasets",
          "version": "2.13.1",
          "source": "PIP"
        },
        {
          "name": "skops",
          "version": "0.7.post0",
          "source": "PIP"
        },
        {
          "name": "nltk",
          "version": "3.8.1",
          "source": "PIP"
        }
      ],
      "report_row_ids": [],
      "version": 3
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}